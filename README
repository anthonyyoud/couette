$Id$
----------------------------------------------------------------------------

DESCRIPTION
-----------
Axisymmetric, hydromagnetic, modulated Couette flow between infinite or finite
cylinders in the small Prandtl number limit.  Time-stepping is via 2nd order
accurate implicit Crank-Nicolson for the linear terms and 2nd order accurate
explicit Adams-Bashforth for the non-linear terms.  The code uses operator
factorisation in solving the diffusive equations to allow a tridiagonal system
to be solved.  The spatial discretisation is via 2nd order accurate centred
finite differences.  It includes a 'homotopy' parameter, tau, to continuously
deform the boundaries from the infinite cylinder case (tau=0) to the finite
cylinder case (tau=1).  A basic particle path subroutine to track the
trajectory of a particle is also included.  Spatial modulation of the inner
and/or outer Reynolds numbers in the axial direction is possible to mimic a
wavy cylinder boundary.

The code uses the linear algebra package ScaLAPACK to enable parallelism in
solving the Poisson equations associated with the stream function, current and
magnetic field.

IMPORTANT INFORMATION
---------------------
Since the code uses ScaLAPACK for the parallelism, this package must be
available.  ScaLAPACK in turn depends on other packages.  These are (in order):

MPI	- A version of the message-passing interface.  This code has been
          tested using LAM-MPI, therefore this would be the recommended version
          to use.  It has been tentatively tested using MPICH, although I did
          not build the required libraries myself.  All I can say is that the
          code does work using MPICH if all the libraries are built correctly. 

BLACS	- The Basic Linear Algebra Communication System.  ScaLAPACK uses this
	  to call the MPI commands, hence no knowledge of MPI is required.

BLAS	- The Basic Linear Algebra Subprograms.  Contrary to other
	  recommendations, the reference version of the BLAS should be used and
	  not a machine optimised version (such as ATLAS or Intel MKL).  Since
	  the matrices in this code are banded, the code does not benefit from
	  the faster Level 3 BLAS routines that an optimised library provides.
          Most of the calls are to Level 2 BLAS.

It is not necessary for LAPACK to be installed.

To ensure that this code compiles, links and runs correctly, it is *highly*
recommended that MPI, BLACS, BLAS and ScaLAPACK are all compiled with the same
Fortran compiler.  The code has been tested and is known to work with the Intel
Fortran Compiler versions 7.0, 7.1, 8.0, and 8.1.  A later version of 7.1
should be used to avoid a compiler bug.  For consistency the Intel C++ Compiler
should also be used when necessary (although gcc will work as well).  This code
should then be compiled using the MPI compiler 'mpif77' which will be a wrapper
for the Intel compiler, ensuring that all the necessary libraries are linked
and the correct flags are set.

Failure to be consistent in the use of a Fortran compiler will almost certainly
result in undefined references to libraries and other routines and missing
symbols at link time, as well as the likelihood that the Fortran to C naming
conventions will be broken.

FILES
-----
couette_mod.f90			- Main program file.  Calculates entire 
				  azimuthal velocity including CCF.

current.f90			- Routines to solve the Poisson equation for
				  the azimuthal current.

derivs.f90			- Routines to calculate derivatives.

ic_bc.f90			- Routines to set up initial and boundary
                                  conditions.

io.f90				- In general, routines to do with 
				  input/output.

linear.f90			- Routines to set up the linear parts of the
                                  RHS of the azimuthal velocity and vorticity
                                  fields.

magnetic.f90			- Routines to solve the Poisson equation for
				  the azimuthal magnetic field.

makefile			- Makefile for use with Intel Fortran 
				  Compiler (see below).

matrices.f90			- Routines to set up matrices involved 
				  in time-stepping and solving Poisson 
				  equations.

nonlinear.f90			- As linear.f90 but for the nonlinear terms.

parameters.f90			- Parameters to set (see below).

README				- This file.

run_script.sh			- For use on giga cluster.  Parameters to 
                                  determine parallelism should be set in 
                                  this file. (see below)

setup				- Setup script to compile and set up 
				  ready to run in separate directory.

solve.f90			- Routines to solve for the azimuthal velocity
                                  and vorticity fields, as well as the Thomas
                                  algorithm.

stream.f90			- Routines to solve Poisson equation for 
				  stream function.

variables.f90			- Types for variables declared, and other
                                  routines to do with variables in general.

MAIN PARAMETERS
---------------
i1		- The intrinsic data types in the code are parametrised, so
		  the precision does not depend on the architecture on which it
		  is run.  The Fortran intrinsic selected_int_kind() is used to
		  select the required integer precision.  The default value of
		  9 corresponds to a standard 4-byte (32-bit) integer on x86
		  machines, where 9 corresponds to the range of integers
                  (-10^9,10^9).

r2		- As for i1 above, the real precision is parametrised using
		  the Fortran intrinsic selected_real_kind(), the default value
		  of which is 15,307 corresponding to a floating point number
		  with a precision of 15 digits and an exponent range of
		  (-307,307).  This corresponds to 8-byte (64-bit) double
                  precision on x86 machines.

nprow		- The number of process rows to use for distribution of the
                  matrices.  As of release 1.7 of ScaLAPACK, this must be set
                  to 1.

npcol		- As above but for the number of process columns.  This can
                  be any integer.  In practice this should not exceed the
                  number of CPU's available.  It can be set to 1 in which
                  case the job will be run on one CPU.  Doing this will
                  guarantee a faster version of the code than the standard
                  serial version.  The matrix orders must be large before
                  any speedup (over a parallel 1-CPU run) is observed in a 
                  multi-CPU run.

nb		- The 'blocking factor' to use when distributing the matrices
                  over the process grid.  See the ScaLAPACK documentation for
                  a full explanation of what this does; basically it
                  determines the size of the matrices on each process.  For
                  banded codes like this there can only be one block per CPU,
                  meaning that this should be set to at least
                  ceiling( (nx+1)*(nz+1) ) / npcol.  This guarantees that each
                  CPU receives only one block of the matrix.

alpha		- Wavenumber (2pi/wavelength) in the infinite case.  Set equal
                  zero if using finite cylinders.

gamma		- Aspect ratio (height/gap) for the finite case.  Set equal to
                  2pi/alpha if using infinite cylinders.

eta		- Radius ratio (R1/R2).

Q		- Chandrasekhar number.  Measure of the imposed magnetic field.

Re1,2		- Reynolds number in Re1,2(t)=Re1,2+Re1,2mod*cos(om1,2*t).

Re1,2_mod	- Modulation amplitude as above.

om1,2		- Frequency of modulation as above.

Re_incr         - How much Re1 should be incremented or decremented when
                  searching for critical values.

growth_tol      - How close two successive growth rates should be before Re1 is
                  altered in searching for critical values.

dt		- Timestep.  In general 10^-4 is a good choice.  Once the
		  Reynolds numbers are large (>700, say) and/or the spatial
                  resolution is increased significantly (>80 points in z) then
                  dt=10^-5 is a better choice for stability.

seed		- Initial seed for initial conditions.  Set small O(10^-10) for
		  calculating linear growth rates.  Set O(10^-3) for non-linear
		  saturation.  In practice this can be set to zero, since the
		  boundary conditions of the azimuthal velocity at the cylinder
                  wall(s) can start the flow.

end_time	- Final viscous diffusion time.

tau_init	- Initial value of homotopy parameter, tau.  0<=tau<=1.
                  tau=0 => infinite cylinders, tau=1 => finite cylinders.

tau_step	- For steady case, how much tau should be increased after
                  saturation at each tau.

tau_end		- For steady case, the final value of tau desired.

nx		- Number of radial grid points.

nt		- Number of azimuthal grid points for use when a 3D OpenDX
		  isosurface is desired.  This is not actually used in any
		  computation in the code other than for the isosurface plots.

nz		- Number of axial grid points.  For infinite 2*nx is
		  sufficient.  For finite gamma*nx is usually required for full
                  resolution.

save_rate	- After how many time-steps should velocities be saved?

save_rate_2	- After how many time-steps should cross-sections be saved?

xsect_save	- Should cross-sections (surfaces) of fields be saved?

save3d		- Should a 3D isosurface be saved (OpenDX)?

iso_hel		- If save3d=.true. should the isosurface be of the helicity?
                  If iso_hel=.false. then the stream function is saved.

restart		- Should we restart from a previous run?  If .true., file
                  'end_state.dat' should be copied to restart directory.

auto_tau	- For steady case, should tau be automatically stepped after
		  saturation at each tau.  Set in conjunction with tau_step and
                  tau_end.

auto_Re         - For steady case, should Re1 be stepped automatically?

dec_Re          - Can the critical value for Re1 only be found by a
                  quasi-static decrease of Re1?

hyst_Re         - Is the critical value of Re1 in a hysteresis region?
                  (Specifically for 1- and 2-cell flows at very small aspect
                  ratio).

RARELY USED PARAMETERS
----------------------
eps1,2		- Amplitude of oscillation of Re_1,2(t,z) in axial direction.

freq1,2		- Frequency of oscillation of Re_1,2(t,z) in axial direction.

x_par_pos	- Initial radial position of a particle in the fluid as a
                  fraction of gap width.

z_par_pos	- Initial axial position of a particle in the fluid as a
                  fraction of gamma (finite) or alpha (infinite).

save_part	- Should a particle path be saved?

RUN_SCRIPT.sh
-------------
This is a bash script that is read whenever a job is submitted to the giga
queue.

SERIAL		- Set to 0 if you want to run the parallel version of the code.
		  Set to 1 if you want to use this script to run the serial
                  version of the code.

NPROCS		- Total number of processes on which the job will be run.  This
                  should be equal to 'npcol' in parameters.f90.

EXE		- The executable that will be run.

DATA		- Any files that will be copied from the submit directory to
                  the work directory.

NODECPU		- The number of CPU's available on each node.  The giga cluster
		  has 24 nodes each with dual hyperthreaded Intel P4 Xeon's at
		  2.8GHz.  Physically, this means there are 2 CPU's per node,
		  and NODECPU should probably be set to 2, unless the job is
		  incredibly large and requires more than 48 CPU's, in which
		  case NODECPU can be set to 4 (giving access to 96 (logical)
		  CPU's).  It should be noted that any code will run faster on
                  physical CPU's than on logical CPU's.

NODELIST	- A list of the nodes that the script will attempt to submit
		  the job to.  There should be no more names here than
		  necessary.  e.g. if NPROCS=16 and NODECPU=2 then you will
		  need to set 8 hostnames; if NODECPU=4 then you will only need
                  4 hostnames.  If NPROCS=1, then this can be left blank.

DATA_DIR	- The directory where all data will be stored.  Set to `pwd` if
                  the submit directory should be used.

RUN_DIR		- The directory where the job will run on the nodes.  This
		  shouldn't need changing.  /work is the space where all
		  submitted jobs should be run, and appending $JOB_ID ensures
		  that the job name is totally unique (to prevent accidentally
                  overwriting previous runs).

MAKEFILE
--------
Settings here will need to be changed, depending on the architecture on which
the code is run.

OUTDIR		- The output directory of the object files and executable.

OBJECTS		- The object files that should be linked.

FFLAGS		- Compiler flags for the Intel Fortran Compiler.  By default
		  these are set to very aggressive optimisations.
		  Specifically, the -tpp7 and -xW flags optimise for P4 CPU's
		  only.  If the code is run on any other CPU, it **will not
		  work**.  If the CPU is not a P4 (including Xeon's) then these
		  two flags should be changed before compilation.  The
		  commented out FFLAGS enables debugging and will give more
		  output in case of an error.  In this case, no optimisation is
                  performed.

LINKFLAGS	- Any extra flags required by the linker (which in most cases
		  will be the Intel compiler) should go here.  Specifically, on
		  some Red Hat systems and systems with certain versions of
		  glibc (I have not checked them all) and using versions 7.0 or
		  7.1 of IFC the -i_dynamic flag is required.  Similarly, for
		  the same systems using versions 8.0 or 8.1 of IFC the -static
                  flag is required.

COMPILER	- This should be set to mpif77.  The actual compiler name can
		  be used here (ifc or ifort) but then all the required MPI
                  libraries will have to be manually specified.

LDBLAS		- The location and name of the BLAS library.

LDSCALA		- The location and name of the ScaLAPACK library.

LDBLACS		- The location and names of the BLACS F77, MPI, and C
                  libraries.

LIBS		- No changes needed.

COMPFLAGS	- No changes needed.

TO RUN
------
Set parameters.f90 and run_script.sh (if on giga cluster) then use:
	source setup <directory>
which compiles code and copies parameters.f90 and run_script.sh and moves
couette_mod.out to <directory>.  ./couette_mod.out runs code.

If using the giga cluster then the command:
	qsub run_script.sh
will submit the job to one (or more) of the nodes of the cluster.

If restarting from a previous run then be sure to set restart=.true. in
parameters.f90, and have the file 'end_state.dat' from the preceding run in the
run/submit directory.

Errors are output if either:
1) end_state.dat exists but restart=.false. or
2) restart=.true. but end_state.dat does not exist.

The file 'nodes.dat' in the run directory will tell you on which nodes the job
is running.  The number towards the end of each line tells you how many
processors are available to use on each node.  This can be altered in
run_script.sh.

'qstat' will tell you on which node the job was submitted and subsequently
distributed, and also the job ID.  All files are copied back to the directory
<submit-directory>/$JOB-ID when the job is finished.

If, at any time during a run, the file 'SAVE' is present in the run directory
(by giving the command 'touch SAVE'), the program will note this and output
data files for contours and surfaces at that time.  Once the data files are
output 'SAVE' is removed.  This can be done at any time and as many times as
desired during the run.

Any run should be cleanly stopped by removing the empty file 'RUNNING' which is
present in the run directory once the job has started.  This avoids having to
use Ctrl-C to interrupt the program.  Using this procedure will ensure that all
run-time files are cleaned up and unneeded directories removed.  It also
ensures that the lam-mpi daemon is halted.

OUTPUT FILES
------------
end_state.dat	- Saves time index, p, time-step, dt, and fields u, Z, psi,
                  current and magnetic field for restart.

energy.dat      - Saves the kinetic energy due to CCF and the total kinetic
                  energy including CCF at each time.

max_psi.dat	- Saves maximum value of psi (and vr, vz) over whole field.
                  Mainly for use with IDL plots.

particle.dat	- Saves the radial and axial position of a particle at each
                  time.

run_script.sh	- For use on giga queue.

time_tau.dat	- If auto_tau=.true. saves time at which each step in tau
                  occured.

torque.dat	- Saves the torques on the inner (G1) and outer cylinders (G2),
                  the torque due to CCF (Gc), and the ratio G1/Gc.  For steady
                  states G1=G2.

u_growth.dat	- Saves time, radial velocity (outflow, inflow), growth rate,
		  axial velocity, stream function, azimuthal velocity,
		  vorticity, azimuthal current and magnetic field, and Reynolds
		  number.  Velocities are saved at the points defined in io.f90
                  - subroutine 'save_growth'.

p*******.dat	- Stream function field saved at the time defined by
		  (time-step)*(number following 'p') in filename.  (Only if
                  xsect_save == .true.).

u*******.dat	- As above but for azimuthal velocity field.

z*******.dat	- As above but for vorticity field.

vr*******.dat	- As above but for radial velocity field.

vz*******.dat	- As above but for axial velocity field.

j*******.dat	- As above but for azimuthal current field.

b*******.dat	- As above but for azimuthal magnetic field.

xsect*******.dat- Cross-sections of all fields except vorticity saved at the
		  time defined as above for use in IDL 'jpeg' routine.  (Only
                  if xsect_save == .true.).

Additional output files if job is run on giga queue.

hostfile	- Contains the hostnames of the giga nodes that the job will be
		  submitted to, as well as the number of CPU's available to use
                  on each node.

nodes.dat	- Similar to hostfile, but also shows the originating node,
                  i.e. the node from which the job is submitted.

run******.log	- Info file; showing the number of processes which the job
		  uses, the boot sequence for LAM MPI (parallelism),
		  information about the current run of the code, a list of the
		  processes which end cleanly (if the run ends cleanly then
		  this file should include the statement 'Ending Process'
		  followed by a process number.  This statement is repeated for
		  all processes).  Any error messages will also be shown here.
                  Finally, the total time for the job to complete is shown.
27/06/2005
