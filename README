NOTE
----
Some of this file refers to running the code in parallel.  This used to be the
case, when ScaLAPACK was used instead of LAPACK, but the overhead of the
parallelism was too great, so it is now purely a serial code.  Ignore any
references to running in parallel.

DESCRIPTION
-----------
Axisymmetric, hydromagnetic, modulated Couette flow between infinite or finite
cylinders in the small Prandtl number limit.  Time-stepping is via 2nd order
accurate implicit Crank-Nicolson for the linear terms and 2nd order accurate
explicit Adams-Bashforth for the non-linear terms.  The code uses operator
factorisation in solving the diffusive equations to allow a tridiagonal system
to be solved.  The spatial discretisation is via 2nd order accurate centred
finite differences.  It includes a 'homotopy' parameter, tau, to continuously
deform the boundaries from the infinite cylinder case (tau=0) to the finite
cylinder case (tau=1).  A basic particle path subroutine to track the
trajectory of a particle is also included.  Spatial modulation of the inner
and/or outer Reynolds numbers in the axial direction is possible to mimic a
wavy cylinder boundary.

The code uses the linear algebra package LAPACK to solve the Poisson equations
associated with the stream function, current and magnetic field.

IMPORTANT INFORMATION
---------------------
Since the code uses LAPACK, this library must be available.  LAPACK in turn
depends on BLAS (Basic Linear Algebra Subprograms).  Contrary to other
recommendations, the reference version of the BLAS should be used and not a
machine optimised version (such as ATLAS or Intel MKL).  Since the matrices in
this code are banded, the code does not benefit from the faster Level 3 BLAS
routines that an optimised library provides.  Most of the calls are to Level 2
BLAS.

To ensure that this code compiles, links and runs correctly, it is *highly*
recommended that BLAS and LAPACK are all compiled with the same Fortran
compiler.  The code has been tested and is known to work with gfortran, sunf95,
and the Intel Fortran Compiler versions 7.0, 7.1, 8.0, and 8.1.  There is a
compiler bug in earlier 7.1 revisions.  For consistency, use a C/C++ compiler
from the same compiler collection, when compiling LAPACK and BLAS.

Failure to be consistent in the use of a Fortran compiler will almost certainly
result in undefined references to libraries and other routines and missing
symbols at link time, as well as the likelihood that the Fortran to C naming
conventions will be broken.

FILES
-----
couette_mod.f90			- Main program file.  Calculates entire 
				  azimuthal velocity including CCF.

current.f90			- Routines to solve the Poisson equation for
				  the azimuthal current.

derivs.f90			- Routines to calculate derivatives.

ic_bc.f90			- Routines to set up initial and boundary
                                  conditions.

io.f90				- In general, routines to do with 
				  input/output.

linear.f90			- Routines to set up the linear parts of the
                                  RHS of the azimuthal velocity and vorticity
                                  fields.

magnetic.f90			- Routines to solve the Poisson equation for
				  the azimuthal magnetic field.

makefile			- Makefile for use with Intel Fortran 
				  Compiler (see below).

matrices.f90			- Routines to set up matrices involved 
				  in time-stepping and solving Poisson 
				  equations.

nonlinear.f90			- As linear.f90 but for the nonlinear terms.

parameters.f90			- Parameters to set (see below).

README				- This file.

run_script.sh			- For use on giga cluster.

setup				- Setup script to compile and set up 
				  ready to run in separate directory.

solve.f90			- Routines to solve for the azimuthal velocity
                                  and vorticity fields, as well as the Thomas
                                  algorithm.

stream.f90			- Routines to solve Poisson equation for 
				  stream function.

variables.f90			- Types for variables declared, and other
                                  routines to do with variables in general.

MAIN PARAMETERS
---------------
alpha		- Wavenumber (2pi/wavelength) in the infinite case.  Set equal
                  zero if using finite cylinders.

gamma		- Aspect ratio (height/gap) for the finite case.  Set equal to
                  2pi/alpha if using infinite cylinders.

eta		- Radius ratio (R1/R2).

Q		- Chandrasekhar number.  Measure of the imposed magnetic field.

Re1,2		- Reynolds number in Re1,2(t)=Re1,2+Re1,2mod*cos(om1,2*t).

Re1,2_mod	- Modulation amplitude as above.

om1,2		- Frequency of modulation as above.

Re_incr         - How much Re1 should be incremented or decremented when
                  searching for critical values.

growth_tol      - How close two successive growth rates should be before Re1 is
                  altered in searching for critical values.

dt		- Timestep.  In general 10^-4 is a good choice.  Once the
		  Reynolds numbers are large (>700, say) and/or the spatial
                  resolution is increased significantly (>80 points in z) then
                  dt=10^-5 is a better choice for stability.

seed		- Initial seed for initial conditions.  Set small O(10^-10) for
		  calculating linear growth rates.  Set O(10^-3) for non-linear
		  saturation.  In practice this can be set to zero, since the
		  boundary conditions of the azimuthal velocity at the cylinder
                  wall(s) can start the flow.

end_time	- Final viscous diffusion time.

tau_init	- Initial value of homotopy parameter, tau.  0<=tau<=1.
                  tau=0 => infinite cylinders, tau=1 => finite cylinders.

tau_step	- For steady case, how much tau should be increased after
                  saturation at each tau.

tau_end		- For steady case, the final value of tau desired.

nx		- Number of radial grid points.

nt		- Number of azimuthal grid points for use when a 3D OpenDX
		  isosurface is desired.  This is not actually used in any
		  computation in the code other than for the isosurface plots.

nz		- Number of axial grid points.  For infinite 2*nx is
		  sufficient.  For finite gamma*nx is usually required for full
                  resolution.

save_rate	- After how many time-steps should velocities be saved?

save_rate_2	- After how many time-steps should cross-sections be saved?

xsect_save	- Should cross-sections (surfaces) of fields be saved?

save3d		- Should a 3D isosurface be saved (OpenDX)?

iso_hel		- If save3d=.true. should the isosurface be of the helicity?
                  If iso_hel=.false. then the stream function is saved.

restart		- Should we restart from a previous run?  If .true., file
                  'end_state.dat' should be copied to restart directory.

auto_tau	- For steady case, should tau be automatically stepped after
		  saturation at each tau.  Set in conjunction with tau_step and
                  tau_end.

auto_Re         - For steady case, should Re1 be stepped automatically?

dec_Re          - Can the critical value for Re1 only be found by a
                  quasi-static decrease of Re1?

hyst_Re         - Is the critical value of Re1 in a hysteresis region?
                  (Specifically for 1- and 2-cell flows at very small aspect
                  ratio).

RARELY USED PARAMETERS
----------------------
eps1,2		- Amplitude of oscillation of Re_1,2(t,z) in axial direction.

freq1,2		- Frequency of oscillation of Re_1,2(t,z) in axial direction.

x_par_pos	- Initial radial position of a particle in the fluid as a
                  fraction of gap width.

z_par_pos	- Initial axial position of a particle in the fluid as a
                  fraction of gamma (finite) or alpha (infinite).

save_part	- Should a particle path be saved?

RUN_SCRIPT.sh (old - from when the code used to be parallel)
----------------------------------------------------------
This is a bash script that is read whenever a job is submitted to the giga
queue.

SERIAL		- Set to 0 if you want to run the parallel version of the code.
		  Set to 1 if you want to use this script to run the serial
                  version of the code.

NPROCS		- Total number of processes on which the job will be run.  This
                  should be equal to 'npcol' in parameters.f90.

EXE		- The executable that will be run.

DATA		- Any files that will be copied from the submit directory to
                  the work directory.

NODECPU		- The number of CPU's available on each node.  The giga cluster
		  has 24 nodes each with dual hyperthreaded Intel P4 Xeon's at
		  2.8GHz.  Physically, this means there are 2 CPU's per node,
		  and NODECPU should probably be set to 2, unless the job is
		  incredibly large and requires more than 48 CPU's, in which
		  case NODECPU can be set to 4 (giving access to 96 (logical)
		  CPU's).  It should be noted that any code will run faster on
                  physical CPU's than on logical CPU's.

NODELIST	- A list of the nodes that the script will attempt to submit
		  the job to.  There should be no more names here than
		  necessary.  e.g. if NPROCS=16 and NODECPU=2 then you will
		  need to set 8 hostnames; if NODECPU=4 then you will only need
                  4 hostnames.  If NPROCS=1, then this can be left blank.

DATA_DIR	- The directory where all data will be stored.  Set to `pwd` if
                  the submit directory should be used.

RUN_DIR		- The directory where the job will run on the nodes.  This
		  shouldn't need changing.  /work is the space where all
		  submitted jobs should be run, and appending $JOB_ID ensures
		  that the job name is totally unique (to prevent accidentally
                  overwriting previous runs).

MAKEFILE
--------
Settings here will need to be changed, depending on the architecture on which
the code is run.

OBJECT    - The compiled executable name.

OBJS  		- The object files that should be linked.

FC        - The Fortran compiler.

FFLAGS		- Compiler flags.

LDLAGS    - Any extra flags required by the linker.

TO RUN
------
Set parameters.f90 and run_script.sh (if on giga cluster) then use:
	source setup <directory>
which compiles code and copies parameters.f90 and run_script.sh and moves
couette_mod.out to <directory>.  ./couette_mod.out runs code.

If using the giga cluster then the command:
	qsub run_script.sh
will submit the job to one (or more) of the nodes of the cluster.

If restarting from a previous run then be sure to set restart=.true. in
parameters.f90, and have the file 'end_state.dat' from the preceding run in the
run/submit directory.

Errors are output if either:
1) end_state.dat exists but restart=.false. or
2) restart=.true. but end_state.dat does not exist.

The file 'nodes.dat' in the run directory will tell you on which nodes the job
is running.  The number towards the end of each line tells you how many
processors are available to use on each node.  This can be altered in
run_script.sh.

'qstat' will tell you on which node the job was submitted and subsequently
distributed, and also the job ID.  All files are copied back to the directory
<submit-directory>/$JOB-ID when the job is finished.

If, at any time during a run, the file 'SAVE' is present in the run directory
(by giving the command 'touch SAVE'), the program will note this and output
data files for contours and surfaces at that time.  Once the data files are
output 'SAVE' is removed.  This can be done at any time and as many times as
desired during the run.

Any run should be cleanly stopped by removing the empty file 'RUNNING' which is
present in the run directory once the job has started.  This avoids having to
use Ctrl-C to interrupt the program.  Using this procedure will ensure that all
run-time files are cleaned up and unneeded directories removed.  It also
ensures that the lam-mpi daemon is halted.

OUTPUT FILES
------------
end_state.dat	- Saves time index, p, time-step, dt, and fields u, Z, psi,
                  current and magnetic field for restart.

energy.dat      - Saves the kinetic energy due to CCF and the total kinetic
                  energy including CCF at each time.

max_psi.dat	- Saves maximum value of psi (and vr, vz) over whole field.
                  Mainly for use with IDL plots.

particle.dat	- Saves the radial and axial position of a particle at each
                  time.

run_script.sh	- For use on giga queue.

time_tau.dat	- If auto_tau=.true. saves time at which each step in tau
                  occured.

torque.dat	- Saves the torques on the inner (G1) and outer cylinders (G2),
                  the torque due to CCF (Gc), and the ratio G1/Gc.  For steady
                  states G1=G2.

u_growth.dat	- Saves time, radial velocity (outflow, inflow), growth rate,
		  axial velocity, stream function, azimuthal velocity,
		  vorticity, azimuthal current and magnetic field, and Reynolds
		  number.  Velocities are saved at the points defined in io.f90
                  - subroutine 'save_growth'.

p*******.dat	- Stream function field saved at the time defined by
		  (time-step)*(number following 'p') in filename.  (Only if
                  xsect_save == .true.).

u*******.dat	- As above but for azimuthal velocity field.

z*******.dat	- As above but for vorticity field.

vr*******.dat	- As above but for radial velocity field.

vz*******.dat	- As above but for axial velocity field.

j*******.dat	- As above but for azimuthal current field.

b*******.dat	- As above but for azimuthal magnetic field.

xsect*******.dat- Cross-sections of all fields except vorticity saved at the
		  time defined as above for use in IDL 'jpeg' routine.  (Only
                  if xsect_save == .true.).

Additional output files if job is run on giga queue.

hostfile	- Contains the hostnames of the giga nodes that the job will be
		  submitted to, as well as the number of CPU's available to use
                  on each node.

nodes.dat	- Similar to hostfile, but also shows the originating node,
                  i.e. the node from which the job is submitted.

run******.log	- Info file; showing the number of processes which the job
		  uses, the boot sequence for LAM MPI (parallelism),
		  information about the current run of the code, a list of the
		  processes which end cleanly (if the run ends cleanly then
		  this file should include the statement 'Ending Process'
		  followed by a process number.  This statement is repeated for
		  all processes).  Any error messages will also be shown here.
                  Finally, the total time for the job to complete is shown.
27/06/2005
